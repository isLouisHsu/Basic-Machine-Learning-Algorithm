{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [Introduction of Dataset](#Introduction-of-Dataset)\n",
    "    - [Summary](#Summary)\n",
    "    - [Brief descriptions](#Brief-descriptions)\n",
    "    - [Data format](#Data-format)\n",
    "- [Introduction of Methods](#Introduction-of-Methods)\n",
    "- [Metric](#Metric)\n",
    "- [Implementation](#Implementation)\n",
    "    - [Import modules](#Import-modules)\n",
    "    - [Function definition](#Function-definition)\n",
    "    - [Experiments](#Experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MovieLens](https://grouplens.org/datasets/movielens/) is the oldest recommendation system. Founded by the GroupLens project team of the School of Computer Science and Engineering, Minnesota University, USA, it is a non-commercial, research-oriented experimental site. Movie Lens mainly uses a combination of Collaborative Filtering and Association Rules to recommend movies of interest to users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MovieLens 100K Dataset](http://files.grouplens.org/datasets/movielens/ml-100k.zip) consists of:\n",
    "- 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "- Each user has rated at least 20 movies. \n",
    "- Simple demographic info for the users (age, gender, occupation, zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ml-data.tar.gz   -- Compressed tar file.\n",
    "        To rebuild the u data files do this:\n",
    "            gunzip ml-data.tar.gz\n",
    "            tar xvf ml-data.tar\n",
    "            mku.sh\n",
    "                \n",
    "- u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
    "        - Each user has rated at least 20 movies.  \n",
    "        - Users and items are numbered consecutively from 1.  \n",
    "        - The data is randomly ordered. \n",
    "        - This is a tab separated list of \n",
    "            user id | item id | rating | timestamp. \n",
    "        - The time stamps are unix seconds since 1/1/1970 UTC   \n",
    "\n",
    "- u.info     -- The number of users, items, and ratings in the u data set.\n",
    "\n",
    "- u.item     -- Information about the items (movies); \n",
    "        This is a tab separated list of\n",
    "            movie id | movie title | release date | video release date |\n",
    "            IMDb URL | unknown | Action | Adventure | Animation |\n",
    "            Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "            Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "            Thriller | War | Western |\n",
    "        The last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several genres at once. The movie ids are the ones used in the u.data data set.\n",
    "\n",
    "- u.genre    -- A list of the genres.\n",
    "\n",
    "- u.user     -- Demographic information about the users; \n",
    "        This is a tab separated list of\n",
    "            user id | age | gender | occupation | zip code\n",
    "        The user ids are the ones used in the u.data data set.\n",
    "              \n",
    "- u.occupation -- A list of the occupations.\n",
    "\n",
    "- u\\*.base & u\\*.test  -- data sets\n",
    "    - The data sets u1.base and u1.test through u5.base and u5.test are 80%/20% splits of the u data into training and test data.\n",
    "    - The data sets ua.base, ua.test, ub.base, and ub.test split the u data into a training set and a test set with exactly 10 ratings per user in the test set.\n",
    "    - Each of u1, ..., u5 have disjoint test sets; this if for 5 fold cross validation (where you repeat your experiment with each training and test set and average the results).\n",
    "    - The sets ua.test and ub.test are disjoint.\n",
    "    - These data sets can be generated from u.data by mku.sh.\n",
    "\n",
    "- allbut.pl  -- The script that generates training and test sets where all but n of a users ratings are in the training data.\n",
    "\n",
    "- mku.sh     -- A shell script to generate all the u data sets from u.data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of `u1.base` is as follows.\n",
    "\n",
    "| user id | movied id | rating | timestamp |\n",
    "| ------- | --- | ------ | ---------- |\n",
    "| 1\t| 1\t| 5\t| 874965758 |\n",
    "| 1\t| 2\t| 3\t| 876893171 |\n",
    "| 1\t| 3\t| 4\t| 878542960 |\n",
    "| 1\t| 4\t| 3\t| 876893119 |\n",
    "| 1\t| 5\t| 3\t| 889751712 |\n",
    "| $\\cdots$ | $\\cdots$ | $\\cdots$ | $\\cdots$ |\n",
    "| 2 |  1  |\t4 |\t888550871 |\n",
    "| 2 |  10 |\t2 |\t888551853 |\n",
    "| 2 |  14 |\t4 |\t888551853 |\n",
    "| 2 |  25 |\t4 |\t888551648 |\n",
    "| 2 | 100 |\t5 |\t888552084 |\n",
    "| $\\cdots$ | $\\cdots$ | $\\cdots$ | $\\cdots$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded as two matrixes $M_{train}, M_{test}$.\n",
    "$$\n",
    "M = \\left[ \\begin{matrix}\n",
    "    r_{11} & \\cdots & r_{1N_{item}} \\\\\n",
    "   \\cdots & \\cdots & \\cdots \\\\\n",
    "    r_{N_{user}1} & \\cdots & r_{N_{user}N_{item}} \\\\\n",
    "\\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "Take Predictions according to **user similarity** as an example.\n",
    "\n",
    "1. Decompose $M_{train}$ using SVD;\n",
    "    $$ M_{train} = U \\Sigma V^T $$\n",
    "    \n",
    "2. Compress matrix;\n",
    "    $$ M'_{train} = M_{train} V' $$\n",
    "    \n",
    "    where\n",
    "    $$ V' = \\left[ \\begin{matrix} v_1 & v_2 & \\cdots & v_k \\end{matrix} \\right] $$\n",
    "    \n",
    "3. Calculate similarity matrix;\n",
    "    $$ s_{ij} = \\frac{{M'_i}^T M'_j}{||M'_i|| || M'_j||} $$\n",
    "    \n",
    "    where $M'_i$ is the $i$th row of matrix $M'$\n",
    "    \n",
    "4. Calculate predicted rating according to similarity.\n",
    "\n",
    "    For example, in order to predict the $r$th user's rating to the $c$th item, choose $n$ users who are with the highest similarity. Then average the ratings of these users and take the average value as the predicted rating.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE is used to estimate predicted results\n",
    "$$ L = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y - \\hat{y})^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        path: {str}\n",
    "    Returns:\n",
    "        user_id: {ndarray(n)}\n",
    "        item_id: {ndarray(n)}\n",
    "        rating:  {ndarray(n)}\n",
    "    \"\"\"\n",
    "    header = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    \n",
    "    df = pd.read_csv(path, sep='\\t', names=header)\n",
    "    n_users = max(df.user_id)\n",
    "    n_items = max(df.item_id)\n",
    "    \n",
    "    print(\"Number of users = %d, Number of movies = %d\" % (n_users, n_items))\n",
    "    user_id = df.loc[:, 'user_id'].values\n",
    "    item_id = df.loc[:, 'item_id'].values\n",
    "    rating  = df.loc[:, 'rating' ].values\n",
    "\n",
    "    return user_id, item_id, rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw data to dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(raw_data, shape, return_sparse=False):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        raw_data: {tuple(3)} user_id, item_id, rating\n",
    "        shape:    {tuple(2)} rows, cols\n",
    "        return_sparse: {bool} \n",
    "    Returns:\n",
    "        m: {ndarray, sparse matrix}\n",
    "    Notes:\n",
    "        convert raw data to matrix(n_users, n_items)\n",
    "    \"\"\"\n",
    "    user_id, item_id, rating = raw_data\n",
    "    m = sp.coo_matrix((rating, (user_id-1, item_id-1)), shape)\n",
    "    if not return_sparse:\n",
    "        m = m.toarray()\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity_matrix(x):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        x: {ndarray(n_users, n_features)}\n",
    "    Returns:\n",
    "        cosine: {float}\n",
    "    \"\"\"\n",
    "    x_normd = x / np.linalg.norm(x, axis=1).reshape(-1, 1)\n",
    "    cosine = x_normd.dot(x_normd.T)\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(gt, pred):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        gt: {ndarray(n_users, n_items)}\n",
    "        pred: {ndarray(n_users, n_items)}\n",
    "    Returns:\n",
    "        score: {float}\n",
    "    Notes:\n",
    "        \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y - \\hat{y})^2}\n",
    "    \"\"\"\n",
    "    mask = gt != 0\n",
    "    err = gt[mask] - pred[mask]\n",
    "    score = np.sqrt(np.mean(err**2))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train_matrix, test_matrix, compress=80, n_keep=50):\n",
    "\n",
    "    # Singular value decomposition\n",
    "    _, _, vh = np.linalg.svd(train_matrix)\n",
    "\n",
    "    # Compress matrix\n",
    "    train_compressed = train_matrix.dot(vh.T[:, : compress])# N x compress\n",
    "\n",
    "    # Calculate similarity matrix\n",
    "    similarity = get_cosine_similarity_matrix(train_compressed)\n",
    "        \n",
    "    prediction = np.zeros_like(test_matrix)             # to preserve the predicted results\n",
    "    to_pred = np.array(np.where(test_matrix != 0))      # the indexes to be predicted, shape(2, n)\n",
    "    \n",
    "    # predict\n",
    "    for i in range(to_pred.shape[1]):\n",
    "\n",
    "        r, c = to_pred[:, i]                            # `r` is the index of user, `c` is the the index of item\n",
    "\n",
    "        id = np.argsort(similarity[r])[::-1]            # sort samples according to similarity in descending order\n",
    "        id = id[1: n_keep + 1]                          # top `n_keep` users\n",
    "        rates = train_matrix[id, c]                     # get the ratings of chosen samples\n",
    "        rates = rates[rates!=0]                         # filter non-zero data\n",
    "\n",
    "        rate = np.mean(rates) if rates.shape[0] != 0 else 0\n",
    "        prediction[r, c] = rate\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict according to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def according_to_user(train_matrix, test_matrix, compress=80, n_keep=50):\n",
    "    prediction = predict(train_matrix, test_matrix, compress, n_keep)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict according to item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def according_to_item(train_matrix, test_matrix, compress=80, n_keep=100):\n",
    "    prediction = predict(train_matrix.T, test_matrix.T, compress, n_keep).T\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train data set and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 943, Number of movies = 1682\n",
      "Number of users = 462, Number of movies = 1591\n"
     ]
    }
   ],
   "source": [
    "N_USERS, N_ITEMS = 943, 1682\n",
    "shape = (N_USERS, N_ITEMS)\n",
    "    \n",
    "train_raw = load('u1.base')\n",
    "train_matrix = get_matrix(train_raw, shape)\n",
    "test_raw  = load('u1.test')\n",
    "test_matrix  = get_matrix(test_raw,  shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict according to user similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]/[1682]\n"
     ]
    }
   ],
   "source": [
    "KEEP_USER = 50\n",
    "STEP = 20\n",
    "\n",
    "comp_list = []; rmse_list = []\n",
    "for n_comp in range(0, N_ITEMS, STEP):\n",
    "    if n_comp % 200 == 0:\n",
    "        print(\"[%d]/[%d]\" % (n_comp + 1, N_ITEMS))\n",
    "    pred_according_to_user = according_to_user(train_matrix, test_matrix, n_comp + 1, KEEP_USER)\n",
    "    comp_list += [n_comp]; rmse_list += [rmse(test_matrix, pred_according_to_user)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(comp_list, rmse_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict according to item similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_ITEM = 100\n",
    "STEP = 20\n",
    "\n",
    "comp_list = []; rmse_list = []\n",
    "for n_comp in range(0, N_ITEMS, STEP):\n",
    "    if n_comp % 200 == 0:\n",
    "        print(\"[%d]/[%d]\" % (n_comp + 1, N_ITEMS))\n",
    "    pred_according_to_user = according_to_item(train_matrix, test_matrix, n_comp + 1, KEEP_ITEM)\n",
    "    comp_list += [n_comp]; rmse_list += [rmse(test_matrix, pred_according_to_user)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(comp_list, rmse_list)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
